# AIStudio To API

[‰∏≠ÊñáÊñáÊ°£](README_CN.md) | English

A tool that wraps Google AI Studio web interface to provide OpenAI API and Gemini API compatible endpoints. The service acts as a proxy, converting API requests to browser interactions with the AI Studio web interface.

> **üëè Acknowledgements**: This project is forked from [ais2api](https://github.com/Ellinav/ais2api) by [Ellinav](https://github.com/Ellinav). We express our sincere gratitude to the original author for creating this excellent foundation.

## ‚ú® Features

- üîÑ **API Compatibility**: Compatible with both OpenAI API and Gemini API formats
- üåê **Web Automation**: Uses browser automation to interact with AI Studio web interface
- üîê **Authentication**: Secure API key-based authentication
- üê≥ **Docker Support**: Easy deployment with Docker and Docker Compose
- üìù **Model Support**: Access to various Gemini models through AI Studio

## üöÄ Quick Start

### üíª Local Development (Windows Only)

1. Clone the repository:
```powershell
git clone https://github.com/iBenzene/AIStudioToAPI.git
cd AIStudioToAPI
```

2. Run the setup script:
```powershell
npm run setup-auth
```

This script will:
- Automatically download the Camoufox browser (a privacy-focused Firefox fork)
- Launch the browser and navigate to AI Studio automatically
- Save your authentication credentials locally

3. Start the service:
```powershell
npm install
npm start
```

The API server will be available at `http://localhost:7860`

### üåê Server Deployment (Linux VPS)

For production deployment on a server (Linux VPS), you need to extract authentication credentials from a Windows machine first.

#### üìù Step 1: Extract Authentication Credentials (on Windows)

1. Clone the repository on a Windows machine:
```powershell
git clone https://github.com/iBenzene/AIStudioToAPI.git
cd AIStudioToAPI
```

2. Run the setup script:
```powershell
npm run setup-auth
```

This will:
- Download Camoufox browser automatically
- Launch the browser and navigate to AI Studio automatically
- Log in with your Google account
- Save authentication credentials to `configs/auth/auth-N.json` (where N is an auto-incremented index starting from 0)

**How it works**: The script uses browser automation to capture your AI Studio session cookies and tokens, storing them securely in a JSON file. The authentication file is named with an auto-incremented index (auth-0.json, auth-1.json, etc.) to support multiple accounts. This allows the API to make authenticated requests to AI Studio without requiring interactive login on the server.

3. Locate the authentication file:
```powershell
ls configs/auth/auth-*.json
```

4. Copy the auth file to your server:
```powershell
scp configs/auth/auth-*.json user@your-server:/path/to/deployment/configs/auth/
```

5. You can now delete the cloned repository from your Windows machine.

#### üö¢ Step 2: Deploy on Server

##### üêã Option 1: Docker Command

```bash
docker run -d \
  --name aistudio-to-api \
  -p 7860:7860 \
  -v /path/to/auth:/app/configs/auth \
  -e API_KEYS=your-api-key-1,your-api-key-2 \
  -e TZ=Asia/Shanghai \
  --restart unless-stopped \
  ghcr.io/ibenzene/aistudio-to-api:latest
```

Parameters:
- `-p 7860:7860`: API server port (if using a reverse proxy, strongly consider `127.0.0.1:7860`)
- `-v /path/to/auth:/app/configs/auth`: Mount directory containing auth files
- `-e API_KEYS`: Comma-separated list of API keys for authentication
- `-e TZ=Asia/Shanghai`: Timezone for logs (optional, defaults to system timezone)

##### üì¶ Option 2: Docker Compose

Create a `docker-compose.yml` file:

```yaml
name: aistudio-to-api

services:
  app:
    image: ghcr.io/ibenzene/aistudio-to-api:latest
    container_name: aistudio-to-api    
    ports:
      - 7860:7860
    restart: unless-stopped
    volumes:
      - ./auth:/app/configs/auth
    environment:
      API_KEYS: your-api-key-1,your-api-key-2
      TZ: Asia/Shanghai  # Timezone for logs (optional)
```

Start the service:
```bash
sudo docker compose up -d
```

View logs:
```bash
sudo docker compose logs -f
```

Stop the service:
```bash
sudo docker compose down
```

##### üåê Step 3 (Optional): Nginx Reverse Proxy

If you need to access via a domain name or want unified management at the reverse proxy layer (e.g., configure HTTPS, load balancing, etc.), you can use Nginx. Here's the recommended configuration:

Create an Nginx configuration file `/etc/nginx/sites-available/aistudio-api`:

```nginx
server {
    listen 80;
    listen [::]:80;  # IPv6 support
    server_name your-domain.com;  # Replace with your domain

    # For HTTPS, uncomment the following lines and configure SSL certificates
    # listen 443 ssl http2;
    # listen [::]:443 ssl http2;  # IPv6 HTTPS
    # ssl_certificate /path/to/your/certificate.crt;
    # ssl_certificate_key /path/to/your/private.key;

    # Client request body size limit (0 = unlimited)
    client_max_body_size 0;

    location / {
        # Reverse proxy to Docker container
        proxy_pass http://127.0.0.1:7860;

        # Critical: Pass real client IP
        # X-Real-IP: Highest priority, contains the real client IP
        proxy_set_header X-Real-IP $remote_addr;
        
        # X-Forwarded-For: Contains the complete proxy chain
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Other necessary proxy headers
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Timeout settings (adapted for long-running AI requests)
        proxy_connect_timeout 600s;
        proxy_send_timeout 600s;
        proxy_read_timeout 600s;

        # Disable buffering to support streaming responses
        proxy_buffering off;
    }
}
```

Enable the configuration and restart Nginx:

```bash
# Create symbolic link to enable site
sudo ln -s /etc/nginx/sites-available/aistudio-api /etc/nginx/sites-enabled/

# Test if configuration is correct
sudo nginx -t

# Restart Nginx
sudo systemctl restart nginx
```

**‚ö† Multi-layer Proxy Configuration (Important)**:

If using multiple Nginx proxies (e.g., Client -> Public Gateway -> Internal Gateway -> App), inner proxies **should NOT override** `X-Real-IP`:

```nginx
# Inner Nginx (internal gateway) configuration example
location / {
    proxy_pass http://127.0.0.1:7860;
    
    # Critical: Pass through upstream X-Real-IP, do NOT override with $remote_addr
    proxy_set_header X-Real-IP $http_x_real_ip;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    # ... other settings
}
```

**Tips**:

- If you configured HTTPS, it's recommended to set environment variable `SECURE_COOKIES=true` to enable secure cookies
- If using HTTP only, keep `SECURE_COOKIES=false` (default) or leave it unset
- Only use `proxy_set_header X-Real-IP $remote_addr;` at the **outermost public-facing gateway**, inner proxies should use `$http_x_real_ip` to pass through

## üì° API Usage

### ü§ñ OpenAI-Compatible API

```bash
curl -X POST http://localhost:7860/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key-1" \
  -d '{
    "model": "gemini-2.5-flash-lite",
    "messages": [
      {
        "role": "user",
        "content": "Hello, how are you?"
      }
    ],
    "stream": false
  }'
```

### ‚ôä Gemini Native API Format

```bash
curl -X POST http://localhost:7860/v1beta/models/gemini-2.5-flash-lite:generateContent \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key-1" \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello, how are you?"
          }
        ]
      }
    ]
  }'
```

### üåä Streaming Response

```bash
# OpenAI Compatible Streaming Response
curl -X POST http://localhost:7860/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key-1" \
  -d '{
    "model": "gemini-2.5-flash-lite",
    "messages": [
      {
        "role": "user",
        "content": "Write a short poem about autumn"
      }
    ],
    "stream": true
  }'
```

```bash
# Gemini Native API Streaming Response
curl -X POST http://localhost:7860/v1beta/models/gemini-2.5-flash-lite:streamGenerateContent?alt=sse \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key-1" \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Write a short poem about autumn"
          }
        ]
      }
    ]
  }'
```

## ‚öôÔ∏è Configuration

### üîß Environment Variables

- `API_KEYS`: Comma-separated list of valid API keys for authentication
- `PORT`: API server port (default: 7860)
- `HOST`: Server listening host address (default: 0.0.0.0)
- `STREAMING_MODE`: Streaming mode (default: `real`), only effective when streaming is enabled for the request.
  - `real`: True streaming - directly forwards streaming responses from AI Studio to client
  - `fake`: Simulated streaming - requests AI Studio in non-streaming mode, then converts the complete response to streaming format for the client
- `SECURE_COOKIES`: Enable secure cookies (HTTPS only)
  - Set to `true`: Only HTTPS connections can login (for production with SSL certificates)
  - Set to `false` or leave unset: Both HTTP and HTTPS can login (default, beginner-friendly)
- `ICON_URL`: Custom favicon URL for the web interface
  - Supports any image format (ICO, PNG, SVG, etc.)
  - Supports any size; common sizes are 16x16, 32x32, 48x48 (ICO/PNG) or vector (SVG)
  - Default: `/AIStudio_icon.svg` (local SVG icon)
  - Example: `https://example.com/favicon.ico`
  - If not set, the default local icon will be used
- `FORCE_THINKING`: Force enable thinking mode for all requests (default: false)
  - When set to `true`, all requests will use thinking mode regardless of client settings
- `FORCE_WEB_SEARCH`: Force enable web search for all requests (default: false)
  - When set to `true`, all requests will include web search capability
- `FORCE_URL_CONTEXT`: Force enable URL context for all requests (default: false)
  - When set to `true`, all requests will include URL context capability

### üß† Model Configuration

Edit `configs/models.json` to customize available models and their settings.

## üìÑ License

This project is a fork of [**ais2api**](https://github.com/Ellinav/ais2api) by [**Ellinav**](https://github.com/Ellinav), and fully adopts the CC BY-NC 4.0 license used by the upstream project. All usage, distribution, and modification activities must comply with all terms of the original license. See the full license text in [LICENSE](LICENSE).

### ¬©Ô∏è Copyright / Attribution

- Original work Copyright ¬© [Ellinav](https://github.com/Ellinav)
- Modifications and additions Copyright ¬© 2024 [iBenzene](https://github.com/iBenzene) and contributors
